{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c5eShtzwZanV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4683dc9f-5734-4bd3-c248-ef7c0788cc54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=f4a7946f3d9dc617a3392ac66a3ce380c668570b184e7c6df8d17716c8f1f214\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, sentence_transformers\n",
            "Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
        "import random\n",
        "import os\n",
        "import gc\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from lightgbm import LGBMClassifier\n",
        "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
        "import tensorflow as tf\n",
        "from transformers import pipeline, set_seed, BertTokenizer, TFBertForSequenceClassification\n",
        "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "!pip install sentence_transformers\n",
        "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN은\n",
        "# 데이터 --> Embedding Layer(특성,단어 임베딩) --> RNNlayer(문장임베딩) --> Dense layer\n",
        "\n",
        "# 생각해보면 임베딩레이어와 RNNlayer는 특성 추출이다.\n",
        "\n",
        "# 추출한거를 문장당 몇개의 숫자를 넘겨주면 그 숫자값을 입력으로 해서 Dense layer에서 분류\n",
        "\n",
        "# 앞단계는 특성추출, 뒷단계는 분류\n",
        "\n",
        "# CNN도 비슷하다?!\n",
        "\n"
      ],
      "metadata": {
        "id": "bcl4f0i2-YmW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "FaDIYT7nYoII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN(Convolutional Neural Network) - 구성요소\n",
        "'''\n",
        "\n",
        "특징 추출\n",
        "\n",
        "1. Convolution Layer (합성곱 계층 ) : 합성곱 연산\n",
        "2. Pooling Layer (풀링 계층) : 풀링 연산\n",
        "줄여서 feature map\n",
        "\n",
        "분류\n",
        "\n",
        "Fully Connected Layer (완전 연결 계층 ) : 물체 분류\n",
        "\n",
        "'''\n",
        "\n",
        "# 다중분류면 softmax함수 단일분류면(이중) sigmoid 함수\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "FPcZecSsACQM",
        "outputId": "41d37c9c-47bc-4f5c-8ac5-c888674b8c4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n특징 추출 \\n\\n1. Convolution Layer (합성곱 계층 ) : 합성곱 연산\\n2. Pooling Layer (풀링 계층) : 풀링 연산\\n줄여서 feature map\\n\\n분류\\n\\nFully Connected Layer (완전 연결 계층 ) : 물체 분류\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-1. 합성곱 계층(Convolution Layer) < 핵심\n",
        "# 역할 : 합성곱 연산을 통해 이미지에서 특징을 추출\n",
        "# 특징(feature) : 물체들을 구벼할 수 있는 성질 예) 윤곽선, 귀의 모양, 얼굴의 형태\n",
        "# 필터(filter, mask, kernel) : 3X3 또는 5X5크기의 행렬(2차원 배열)\n",
        "# - 원본이미지를 변환한다의 의미, 필터\n",
        "# - 가린다는 마스크,\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b4i-8bc6BEsi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 합성곱 연산 후 이미지의 크기\n",
        "\n",
        "# 입력 이미지의 크기 (H, W)\n",
        "# 필터의 크기 = (FH, FW)\n",
        "# 특성맵의 크기 = (OH, OW)\n",
        "\n",
        "# -->\n",
        "\n",
        "# OH = (H - FH) +1\n",
        "# OW = (W - FW) +1\n",
        "'''\n",
        "\n",
        "strides가 2일때와 비교하면 1일때보다 결과값이 stirides가 2일 때, 정확하게 2배로 줄어든다.\\\n",
        "\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TD2Hfhi5Jp3Z",
        "outputId": "46a13011-ff50-45bb-9aa4-361b2557de8c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nstrides가 2일때와 비교하면 1일때보다 결과값이 stirides가 2일 때, 정확하게 2배로 줄어든다.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 컬러라서 3채널임.\n",
        "\n",
        "# 입력 채널이 3채널이면 필터도 3채널이어야 한다.\n",
        "\n",
        "# - 입력의 채널 수와 필터의 채널 수가 같아야 함.,\n",
        "\n",
        "# (4,4,3) (3,3,3) = (1+1) (1+1)  Feature맵은 2차원으로 나옴."
      ],
      "metadata": {
        "id": "LJOkMkRaP518"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3차원 데이터의 합성곱 : 블록(cube)으로 생각하기\n",
        "# 입력 데이터와 필터를 아래의 그림처럼 직육면체의 블록으로 생각하면 쉽다.\n",
        "# 3차원 데이터의 모양은 (높이, 너비, 채널) = (Height, Width, Channel) 순으로 표현한다.\n",
        "\n",
        "# # 필터수만큼 피쳐맵이 나온다."
      ],
      "metadata": {
        "id": "C8XohO-MSCOv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenCV와 딥러닝 중 어떤 필터가 최선일까?\n",
        "\n",
        "# 1 OpenCV : 가중치(필터의 숫자 값)을 사람이 결정함.\n",
        "# 2 딥러닝 : 랜덤한 숫자 값에서 출발\n",
        "# -> 학습을 통해 최적의 가중치를 잦는다.\n",
        "# -> 딥러닝에서 학습을 한다는 것은 필터의 가중치 값을 찾는다는 것이다."
      ],
      "metadata": {
        "id": "i3IpDEDGX9GH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 패딩"
      ],
      "metadata": {
        "id": "of20Lil6b_Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩(padding)\n",
        "# - 개념 : 합성곱 연산을 수행하기 전에 입력데이터 주변을 특정 값(주로 0)으로 채워 넣는 것\n",
        "# - 목적 : feature map(특징 맵)의 크기를 입력 데이터의 크기와 동일하게 만드는 것.\n",
        "\n",
        "# (4,4) padding (6,6)\n",
        "# (3,3)을 기준으로 convoluation 연산의 결과 (4,4)이 됨.\n",
        "\n",
        "# 여기서 feature map에서 다시 padding. 2차 feature_map도 (4,4)가 됨.\n",
        "\n",
        "# 앞에서 봤던 특성들이 아닌 다른 특성들을 더 뽑을 수 있을때까지 돌려보는 것.\n",
        "\n"
      ],
      "metadata": {
        "id": "RyaMIaPNYfPg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 풀링(Pooling)"
      ],
      "metadata": {
        "id": "euFzOESjcAyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "1. 풀링(Pooling)\n",
        "개념 : feature map(특징 맵)의 가로, 세로 방향의 크기를 줄이는 연산\n",
        "종류 : Max Pooling, Average Pooling\n",
        "효과 : 데이터 압축, 불필요한 노이즈 제거\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "# Windows Size와 Strides 똑같이 연동돼서 간다!!\n",
        "\n",
        "# (4,4) -> (2,2) # 픽셀값이 4분의1로 줄어든다? 4x4=16. 2x2=4 ...\n",
        "\n",
        "# 불필요한 노이즈가 제거된다, 데이터압축\n",
        "\n",
        "# 입력 신호의 값을 그대로 출력하는 것이 아니라"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "PK5YIiK_cB_7",
        "outputId": "1b228b47-a3ea-4266-ab4f-be667ec38224"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n1. 풀링(Pooling)\\n개념 : feature map(특징 맵)의 가로, 세로 방향의 크기를 줄이는 연산\\n종류 : Max Pooling, Average Pooling\\n효과 : 데이터 압축, 불필요한 노이즈 제거\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  64개가 나왔다. (7,7)\n",
        "\n",
        "#  3,136개의 숫자가 안에 들어 있다.\n",
        "\n",
        "#  3136+1 (깍두기는 꼭 들어감 절편에 해당하는 값)\n",
        "#  거기에 해당하는 가중치가 w1~w3136 + b\n",
        "\n"
      ],
      "metadata": {
        "id": "KpOBkL3ye_Kp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 마지막 처리. 출력층의 활성화 함수 - 이진분류\n",
        "\n",
        "'''\n",
        "목적에 따라 이진분류, 다중분류 활성화함수를 다르게 선택하면 되는 것이고,\n",
        "\n",
        "딱 0일 때 1/2이다.\n",
        "\n",
        "0이상이면 >_1/2 그냥 1로 수렴시킴\n",
        "\n",
        "0이하이면 <-1/2 그냥 0으로 수렴시킴\n",
        "\n",
        "'''\n",
        "### !!! 이미지분류나 텍스트분류나 특성추출 RNN CNN은 다르지만\n",
        "\n",
        "# 그 결과값은 넘어와서 Dense layer에서 처리되기는 마찬가지다!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "7eEsRk0qfmz5",
        "outputId": "39eb804b-629c-4ec1-bbea-a1a5f4e7730f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n목적에 따라 이진분류, 다중분류 활성화함수를 다르게 선택하면 되는 것이고,\\n\\n딱 0일 때 1/2이다. \\n\\n0이상이면 >_1/2 그냥 1로 수렴시킴\\n\\n0이하이면 <-1/2 그냥 0으로 수렴시킴\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력층의 활성화 함수 - 다중분류\n",
        "'''\n",
        "1.3                   0.02\n",
        "5.1                   0.90\n",
        "2.2                   0.05\n",
        "0.7                   0.01\n",
        "1.1                   0.02\n",
        "우측값의 합이 1.9\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "k6IVmwR0hh9Y",
        "outputId": "30adb64b-f079-41b6-c250-a86bba957fa1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1.3                   0.02\\n5.1                   0.90\\n2.2                   0.05\\n0.7                   0.01\\n1.1                   0.02\\n우측값의 합이 1.9\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 은닉층의 활성화 함수\n",
        "'''\n",
        "\n",
        "ReLU 함수 : 입력 값이 음수면 0으로 비활성화하고, 입력 값이 양수면 해당 값을 그대로 출력\n",
        "            y=x 입력이 그대로 출력, 입력의 총합이 0 이상일 때.\n",
        "\n",
        "\n",
        "tanh(하이퍼블릭 탄젠트) 함수 : 시그모이드 함수와 비슷, 출력 값이 -1과 1사이에 존재\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Kg-62kyGlaT9",
        "outputId": "fe086306-31ff-4732-cd1e-1e0650d30b64"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nReLU 함수 : 입력 값이 음수면 0으로 비활성화하고, 입력 값이 양수면 해당 값을 그대로 출력\\n            y=x 입력이 그대로 출력, 입력의 총합이 0 이상일 때.\\n\\n\\ntanh(하이퍼블릭 탄젠트) 함수 : 시그모이드 함수와 비슷, 출력 값이 -1과 1사이에 존재\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tensorflow-keras로 CNN 구현하기\n"
      ],
      "metadata": {
        "id": "KeAMco45m8rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RGB input Image --> tf.keras.layers.Conv2D() --> tf.keras.layers.MaxPool2D()\n",
        "#                                                 tf.keras.layers.AveragePooling2D()\n",
        "#                                                 tf.keras.layers.Dense()\n",
        "\n",
        "'''\n",
        "\n",
        "Conv2D에 ReLu함수 사용 tanh 사용\n",
        "\n",
        "Dense에서는 이진이면 Sigmoid 다중이면 SoftMax 사용\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jJOBDO4CoM7k",
        "outputId": "3ec088e6-a8b7-4d4e-8b52-27b0efd5e01b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nConv2D에 ReLu함수 사용 tanh 사용 \\n\\nDense에서는 이진이면 Sigmoid 다중이면 SoftMax 사용\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN 모델을 이용한 이미지 분류 실습"
      ],
      "metadata": {
        "id": "GcYk_FsmpjSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras에서 제공하는 MNIST dataset을 이용하여 손글씨 이미지를 분류하는 실습을 진행함.\n",
        "\n",
        "# 1) 손으로 쓰여진 0부터 9까지의 숫자 이미지들로 구성\n",
        "# 2) 흑백 이미지, 모양 = (28,28)\n",
        "# 3) 전체 데이터 셋 = train data 60,000개 + test data 10,000개\n"
      ],
      "metadata": {
        "id": "ubPOgrFzpeLb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "CNN 모델 구성 소개\n",
        "\n",
        "입력이미지 - Conv2D(ReLU) - MaxPooling 2D(ReLU) - Conv2D(은닉충-ReLU) - Maxpoolling2D(Softmax) - Dense - 출력(예측)\n",
        "\n",
        "'''\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# add() 함수를 이용하여 레이어 추가 -> model.add(매개변수 = layer)\n",
        "# model.add(tf.keras.layers.Conv2D())\n",
        "# model.add(tf.keras.layers.MaxPool2D())\n",
        "# model.add(tf.keras.layers.flatten())\n",
        "# model.add(tf.keras.layers.Dense())"
      ],
      "metadata": {
        "id": "Z__SLgAkpyCH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 함수 사용법 : Conv2D() 함수\n",
        "# 용도 : 컨볼루션 계층을 생성하는 삼수\n",
        "\n",
        "'''\n",
        "tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=1, padding='same', activation='ReLU', input_shape=(28,28,1))\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "# 2D의 의미는 필터가 두방향으로 움직이니까 그 방향성 때문에 2D이다.\n",
        "# Conv1D는 한방향으로 특성을 추출한다! <얘는 임베딩 벡터값에 대해서 컨볼루션 특성을 추출."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GxSOrKvKr2-6",
        "outputId": "9d8aed14-41b9-4d5b-a1fa-e51fc9c03d77"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=1, padding='same', activation='ReLU', input_shape=(28,28,1))\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conv2D, MaxPool2D Conv2D Maxpool2D 두번 반복해서 쓸지\n",
        "# 한번만 해서 쓸지 알아서 결정할 것.\n",
        "\n"
      ],
      "metadata": {
        "id": "6kbwcRylrZY5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten() 함수\n",
        "# Feature맵을 1차원으로 변환하는 함수\n",
        "\n",
        "# tf.kears.layers.Flatten() --> lstm의 마지막 형태랑 비슷함.\n",
        "\n",
        "'''\n",
        "\n",
        "Dense(units = 10, activation='softmax') / Dense(units = 1, activation='sigmoid')\n",
        "\n",
        "'''\n",
        "\n",
        "# 출력유닛이 하나일때만 SIgmoid, 2개이상일 경우엔 Softmax로 덴스레이어에서 사용\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vMr59dDkRZBA",
        "outputId": "c7d084b0-11b2-40dc-d619-db7e084a6268"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nDense(units = 10, activation='softmax') / Dense(units = 1, activation='sigmoid')\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing"
      ],
      "metadata": {
        "id": "oPFqrJ0eTRV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset 생성하기\n",
        "\n",
        "tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# 실습전 마지막 전처리\n",
        "'''\n",
        "\n",
        "이미지의 픽셀 값을 0과 1사이로 변경\n",
        "1) 이미지 : 0~ 255 사이의 숫자로 존재\n",
        "2) 문제점 : 숫자 크기의 차이로 인해서 학습 시 왜곡이 발생 ( Scaling = Normalizing )\n",
        "\n",
        "'''\n",
        "\n",
        "# Scaling = Normalizing\n",
        "\n",
        "''' 0과 255사이의 숫자를 0과 1 사이의 숫자로 변경시켜주는 행위 '''\n",
        "\n",
        "# 혼공머신 446 페이지"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "niaiTmANTUga",
        "outputId": "64e37e0c-7d8d-4f4b-e540-b22ab505af6b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 0과 255사이의 숫자를 0과 1 사이의 숫자로 변경시켜주는 행위 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# 커널의 크기는 파라미터임. 따라서 여러 가지 값을 시도해 봐야 함. 보통 (3,3)이나 (5,5)크기가 권장됨.\n",
        "\n",
        "keras.layers.Conv2D(10, kernel_size=(3,3), activation='relu', padding='same', strides=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybWvVgY5ceBQ",
        "outputId": "50525eac-ba6c-45ff-fd18-7b64580d1123"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.layers.convolutional.conv2d.Conv2D at 0x7bf574e62650>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 패션 MNIST 데이터 불러오기"
      ],
      "metadata": {
        "id": "xKv1VLXSqeRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 완전 연결 신경망에서는 입력 이미지를 밀집층에 연결하기 위해 일렬로 펼쳐야한다.\n",
        "# 이 작업을 위해 넘파이 reshape() 메서드를 사용하거나 Flatten 클래스를 사용했습니다.\n",
        "# 합성곱 신경망은 2차원 이미지를 그대로 사용하기 때문에 일렬로 펼치지 않습니다.\n",
        "\n",
        "# 입력 이미지는 항상 깊이(채널)이 있어야 한다.\n",
        "# 흑백 이미지의 경우 채널 차원이 없는 2차원 배열이지만 Conv2D층을 사용하기 위해 마지막에 이 채널 자원을\n",
        "# 추가해야 한다. 넘파이 reshape() 메서드를 사용해 전체 배열 차원을 그대로 유지하면서 마지막에 차원을\n",
        "# 간단히 추가할 수 있음."
      ],
      "metadata": {
        "id": "uWGT7AbTqgM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(train_input, train_target), (test_input, test_target) =\\\n",
        "    keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "train_scaled = train_input.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "train_scaled, val_scaled, train_target, val_target = train_test_split(\n",
        "    train_scaled, train_target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsB3HihdrF8p",
        "outputId": "563ec0e9-7b9d-4d81-c61c-0dc243d29604"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    }
  ]
}