{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq2seq 모델\n",
    "#\n",
    "#두 개의 순환 신경망(rnn)로 이루어진 학습 모델\n",
    "#- encoder : 입력값을 받아 이를 고정된 크기의 문맥 벡터(context vector)로 변환(읽기)\n",
    "#- decoder : 문맥 벡터를 사용해서 출력 값 생성(쓰기)\n",
    "#\n",
    "#하나의 텍스트 문장이 입력으로 들어오면 하나의 텍스트 문장을 출력하는 구조, 생성 모델, 기계 번역에서 사용되는 모델\n",
    "#\n",
    "#원래 구글 번역기의 모델, 하지만 지금은 더 발전했음.\n",
    "\n",
    "# 인코더에 입력이 있고 디코더에도 입력이 있다. 양쪽 모두 다 입력이 있음.\n",
    "# 디코더의 입력과 인코더의 입력은 다르다.\n",
    "# 인코더의 입력은 한번에 다준다. 우리가 아는대로 RNN을 거치면서 문장임베딩으로 바뀜\n",
    "# 디코더의 입력은 하나만 준다. 하나씩도 아니고. 한개만 줌..왜,.?\n",
    "\n",
    "# 쨌든 성능이 잘 안나와서 트랜스포머가 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer \n",
    "#\n",
    "#- 순환 신경망의 한계\n",
    "#\n",
    "#이전 시점의 정보를 기억하고, 현재 시점의 입력과 함께 다음 시점의 출력을 생성하는 구조\n",
    "#=> 앞을 보고 뒤를 맞추는 구조\n",
    "#\n",
    "#문장의 길이가 길수록 모든 정보를 하나의 벡터에 포함하기에 부족(앞부분 단어 정보 손실)\n",
    "#\n",
    "#입력 시퀀스의 순서에 따라 순차적으로 연산을 수행하기 때문에 병렬 처리가 어려움.\n",
    "#\n",
    "#이는 RNN의 학습과 추론 속도를 느리게 하고, 대규모 데이터와 모델을 다루기 어렵게 만듦.\n",
    "#\n",
    "#- Transformer\n",
    "#\n",
    "#2017년 구글이 발표한 논문(Attention is all you need)\n",
    "#순환 신경망의 한계를 극복하기 위해 제안된 모델\n",
    "#RNN을 사용하지 않고, 어텐션(attention) 알고리즘을 통해 시퀀스 데이터를 처리하는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 트랜스포머의 핵심은 self-attention\n",
    "#\n",
    "#입력 문장의 각 단어가 다른 단어들과 얼마나 관련이 있는지를 계산하고, \n",
    "#\n",
    "#그 결과를 가중합하여 새로운 단어 임베딩 벡터를 생성하는 알고리즘\n",
    "#\n",
    "#과정1 : 입력 문장 -> 토큰화 + 임베딩 + positional encoding\n",
    "\n",
    "# LSTM은 한단어씩 처리하니 순서가 명확한데 얘는 한꺼번에 다 넣고 처리한다. 단어에 족보가 없다. 순서가\n",
    "\n",
    "# 가려지지 않는다. 그래서 임베딩에다가 position encoding 숫자 하나를 더 집어넣는다\n",
    "\n",
    "### 위치정보가 반영된 임베딩 팩터를 만든다!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 과정 2 : 각 단어 임베딩 벡터 -> 쿼리(query), 키(key), 값(value) 벡터 생성\n",
    "#\n",
    "#쿼리(query), 키(key), 값(value) 벡터 간의 영상을 통해서 문맥적 관계성을 추출하는 과정\n",
    "\n",
    "# 행렬 곱 --> '내적' 연속\n",
    "\n",
    "#1) query : 문장 내 다른 단어들과 어떤 관계를 가지는지를 파악하고자 하는 특정 단어의 벡터\n",
    "\n",
    "# 각 단어들만의 query벡터가 존재.\n",
    "\n",
    "#2) key : query가 문장 내 어떤 단어와 유사도가 큰 지를 계산하는 데 사용되는 단어 벡터\n",
    "#--> 입력 문장의 전체 단어 벡터\n",
    "#\n",
    "\n",
    "#나머지 단어의 key벡터와 존재. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\human\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 필요한 라이브러리 임폴트\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.5110626   0.42292204 -0.41969493 ... -1.1673577   0.777814\n",
      "   0.58657396]\n",
      " [-0.13087033 -0.4497122   3.3774817  ...  0.2500961  -0.69026154\n",
      "  -0.8148735 ]\n",
      " [ 0.40156764  0.3129424  -0.87114996 ... -0.3053926   0.18731174\n",
      "  -1.6565207 ]], shape=(3, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "### 실습용 데이터 생성\n",
    "\n",
    "'''\n",
    "1. sentence = 'i love you'\n",
    "2. 토큰화 --> [i, love, you]\n",
    "3. 각 토큰 --> 512 차원의 임베딩 벡터로 변환 --> (3, 512) 임베딩 행렬 생성\n",
    "4. 표준 정규 분포로부터 랜덤한 실수 샘플링 --> (3, 512) 임베딩 행렬에 해당하는 데이터 생성\n",
    "'''\n",
    "\n",
    "# 랜덤 시드 설정\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# (3, 512) 임베딩 행렬에 해당하는 데이터 생성\n",
    "embedding_shape = (3, 512) #np.random.normal 하지만 텐소플로우에서는 넘파이배열을 쓰지 않음. (표준정규분포곡선만들기)\n",
    "embeddings = tf.random.normal(shape=embedding_shape)\n",
    "\n",
    "# 결과 확인하기\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 가중치 행렬의 모양 : [ 512 1536]\n",
      "가중치 행렬 W_q의 모양 : [512 512]\n",
      "********************************************************************************\n",
      "가중치 W_k의 모양 : [512 512]\n",
      "********************************************************************************\n",
      "가중치 W-v의 모양 : [512 512]\n"
     ]
    }
   ],
   "source": [
    "### 가중치 행렬 W_q, W_k, W_v (Weights : query, key, value) \n",
    "\n",
    "'''\n",
    "1. 가중치 행렬 W_q, W_k, W_v의 모양 : (512, 512)\n",
    "2. 가중치 행렬의 초기 값 --> 랜덤한 실수로 설정\n",
    "3. 표준 정규 분포로부터 랜덤한 실수 샘플링 --> (512, 512) 가중치 행렬에 해당하는 데이터 W_q, W_k, W_v 생성\n",
    "'''\n",
    "\n",
    "# (512, 512*3) 모양의 가중치 행렬 생성\n",
    "weights_shape = (512, 512*3)\n",
    "weights = tf.random.normal(shape=weights_shape)\n",
    "print(f'생성된 가중치 행렬의 모양 : {tf.shape(weights)}')\n",
    "\n",
    "# 쿼리를 만드는 가중치행렬값은 0~512\n",
    "# 키를 만드는 가중치행렬값은 512~1024\n",
    "# 밸류를 만드는 가중치행렬값은 1024~1536\n",
    "\n",
    "# 가중치 행렬 --> 분할 --> 모양 : (512, 512)\n",
    "W_q = weights[:, 0:512]\n",
    "W_k = weights[:, 512:1024]\n",
    "W_v = weights[:, 1024:1536]\n",
    "\n",
    "print(f'가중치 행렬 W_q의 모양 : {tf.shape(W_q)}')\n",
    "\n",
    "print('*'*80)\n",
    "\n",
    "print(f'가중치 W_k의 모양 : {tf.shape(W_k)}')\n",
    "\n",
    "print('*'*80)\n",
    "\n",
    "print(f'가중치 W-v의 모양 : {tf.shape(W_v)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.0668802   0.19454929 -0.53082895 ...  0.28124705 -0.41999227\n",
      "  -1.731296  ]\n",
      " [-0.88284314  0.07581621  0.36982587 ... -0.98544943 -0.07025491\n",
      "  -0.35464865]\n",
      " [-1.4630128   1.008406    0.00408987 ...  2.3250976  -1.2281883\n",
      "  -0.36647767]\n",
      " ...\n",
      " [ 0.10821439 -0.23559454 -0.17931989 ... -0.1931868   0.06187644\n",
      "   0.02346488]\n",
      " [ 0.17702705  1.1367483   0.44510984 ...  0.46767908  2.3448489\n",
      "  -0.61274064]\n",
      " [-0.32561806  0.1451457  -0.6740147  ...  0.5818147  -1.1355666\n",
      "   0.1629009 ]], shape=(512, 1536), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 qkv의 모양 : \n",
      "[   3 1536]\n",
      "********************************************************************************\n",
      "생성된 q 행렬의 모양 : \n",
      "[  3 512]\n",
      "********************************************************************************\n",
      "생성된 k 행렬의 모양 : \n",
      "[  3 512]\n",
      "********************************************************************************\n",
      "생성된 v 행렬의 모양 : \n",
      "[  3 512]\n"
     ]
    }
   ],
   "source": [
    "### q, k, v 생성\n",
    "\n",
    "'''\n",
    "### 임베딩 행렬과 가중치 행렬곱 --> 각 단어의 q, k, v 백터 생성\n",
    "'''\n",
    "\n",
    "# 전체 단어의 qkv 행렬 생성(linear algebra, 선형대수)\n",
    "qkv = tf.linalg.matmul(a=embeddings, b=weights)\n",
    "\n",
    "# 각 단어의 q, k, v 추출\n",
    "q = qkv[:, 0:512]\n",
    "k = qkv[:, 512:1024]\n",
    "v = qkv[:, 1024:]\n",
    "\n",
    "\n",
    "# 결과 확인하기\n",
    "print(f'생성된 qkv의 모양 : \\n{tf.shape(qkv)}')\n",
    "\n",
    "print('*'*80)\n",
    "\n",
    "print(f'생성된 q 행렬의 모양 : \\n{tf.shape(q)}')\n",
    "\n",
    "print('*'*80)\n",
    "\n",
    "print(f'생성된 k 행렬의 모양 : \\n{tf.shape(k)}')\n",
    "\n",
    "print('*'*80)\n",
    "\n",
    "print(f'생성된 v 행렬의 모양 : \\n{tf.shape(v)}')\n",
    "\n",
    "# i love you (query) i love you (query) 한번에 표를 만들어서 한번에 때려박아서 진행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[22.627417]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### scaled_dot_product_attention 함수 정의 \n",
    "# 벡터간의 곱 = 내적 matrix multiply ( matmul ) 행렬간의 곱 = 벡터간의 곱\n",
    "# 행렬을 구성하고 있는 성분 벡터,와 벡터간의 곱. 행렬곱이란 벡터간의 내적.\n",
    "# 벡터 간의 내적은 a.b a*b로 표현되기 때문에 adotproduct.\n",
    "\n",
    "'''\n",
    "transpose(전치하다) ex) (3,512) (3,512) 안맞으니 바꿔줘야함 (3,512) (512,3) K의T승 -> 뒤집어라라는 뜻.\n",
    "self-attention\n",
    "과정 3 : Scaled Doct-Product Attention\n",
    "1) attention score --> query 벡터와 key벡터의 내적을 통해 각 단어 간의 유사도 점수를 계산\n",
    "2) scaling : attention score 값 / key 벡터의 차원 크기(열의숫자, 특성의숫자, feature numbers=512, 나중에 매개변수로\n",
    "지정가능)의 제곱근 -> attention score 값의 크기를 조정 -> 숫자의 범위를 조정.\n",
    "3) attention weight(어텐션 가중치)\n",
    "\t1 스케일링을 거친 attention score를 softmax 함수에 입력\n",
    "\t2 입력 문장의 각 단어가 다른 단어들과 얼마나 관련이 있는지를 0과 1사이의 숫자로 표현\n",
    "\n",
    "4) 어텐션 출력(attention output) 벡터 생성\n",
    "1 어텐션 가중치(attention weight)와 v벡터의 가중합 계산\n",
    "2 문장을 구성하고 있는 각 단어 간의 관계를 반영한 새로운 단어 임베딩 벡터 생성\n",
    "\n",
    "\n",
    "대각선이 1이 안나오는 대신에 가장 큰 숫자다.\n",
    "\n",
    "'''\n",
    "# query와 key간의 전치를 통해서 내적을 구했다 까지. 내적을 구하면 Self-attention을 구할 수 있다.\n",
    "\n",
    "#열심히 안해도됨이건. 그냥 예시임\n",
    "num = np.array([512.0])\n",
    "num = tf.reshape(num, [1,1])\n",
    "\n",
    "tf.math.sqrt(num)\n",
    "\n",
    "# soft max는 0.xx지만 다 더하면 1이 된다.\n",
    "# soft max 자체가 확률을 얘기하는 것이다.\n",
    "# 따라서 언어모델은 다중분류. softmax 함수를 써서 각각의 단어가 나올 확률처럼 확률을 알려준다!\n",
    "# 정답의 컬럼이 5개다. 입력의 총합은 1이 아님 확률이 아니기 때문. Softmax 거쳐서 나온 결과값이 확률임.\n",
    "# 분모는 입력값의 전체. 분자는 이중의 하나만 가지고 계산\n",
    "\n",
    "# 입력값이 클수록 결과값도 커진다! 정비례라곤 할 수 없음!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### scaled_dot_product_attention 함수 정의\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v):\n",
    "    attention_score = tf.linalg.matmul(a=q, b=k, transpose_b=True)\n",
    "    print(f'attention_score : \\n{attention_score}')\n",
    "    \n",
    "    print('*'*80)\n",
    "    \n",
    "    # q과 k간의 행렬곱 matmul_qk을 했는데 범위조절을 좀 하겠다. 너무 크다..\n",
    "    # scale matmul_qk\n",
    "    dk=tf.cast(k.shape[1], tf.float32)\n",
    "    scaled_attention_score = attention_score / tf.math.sqrt(dk) # key의 차원수 dk (성분원소가 몇개냐 = 차원수)\n",
    "\t# 한쪽은 실수, 한쪽은 정수. 넘파이에선 문제가 안되나, 텐서플로우에선 아직 문제가 됨.\n",
    "\t# k.shape[1] 자료형을 실수로 바꿔줘야함. pandas에선 astype, tf에선 cast(자료형 변환)\n",
    "    print(f'scaled_attention_score : \\n{scaled_attention_score}')\n",
    "    print('*'*80)\n",
    "\n",
    "\t# softmax 함수 시행\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_score)\n",
    "    print(f'attention_weights : \\n{attention_weights}')\n",
    "    print('*'*80)\n",
    "    # attention output 생성 --> 가중치 * value\n",
    "    output = tf.linalg.matmul(a=attention_weights, b=v) # (3.3) * (3.512)\n",
    "    \n",
    "    return output #새로운 단어 임베딩 팩터 생성, 가중치까지 생성해보장 ㅎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_score : \n",
      "[[-18987.39     6699.8164  -9973.195 ]\n",
      " [ 20949.95    21679.52     1974.8228]\n",
      " [  3651.5854  23628.8     -6038.951 ]]\n",
      "********************************************************************************\n",
      "scaled_attention_score : \n",
      "[[-839.1321    296.09286  -440.75714 ]\n",
      " [ 925.8657    958.10846    87.275665]\n",
      " [ 161.3788   1044.2554   -266.88647 ]]\n",
      "********************************************************************************\n",
      "attention_weights : \n",
      "[[0.00000e+00 1.00000e+00 0.00000e+00]\n",
      " [9.93476e-15 1.00000e+00 0.00000e+00]\n",
      " [0.00000e+00 1.00000e+00 0.00000e+00]]\n",
      "********************************************************************************\n",
      "각 단어별 새로 생성된 임베딩 행렬 : \n",
      "[[-33.50708    27.014906   -2.748849  ...  63.532726   -0.5750847\n",
      "    6.839353 ]\n",
      " [-33.50708    27.014906   -2.748849  ...  63.532726   -0.5750847\n",
      "    6.839353 ]\n",
      " [-33.50708    27.014906   -2.748849  ...  63.532726   -0.5750847\n",
      "    6.839353 ]]\n",
      "********************************************************************************\n",
      "각 단어별 새로 생성된 임베딩 행렬의 모양 : (3, 512)\n"
     ]
    }
   ],
   "source": [
    "### scaled_dot_product_attention 함수 실행\n",
    "embeddings = scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "# 결과 확인하기\n",
    "print(f'각 단어별 새로 생성된 임베딩 행렬 : \\n{embeddings}')\n",
    "\n",
    "print('*'*80)\n",
    "print(f'각 단어별 새로 생성된 임베딩 행렬의 모양 : {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT(Generative Pre-Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 2018년에 OpenAI가 공개한 언어 모델\n",
    "#\n",
    "#언어 모델 : 이전 단어들이 주어졌을 때 다음 단어로 어떤 단어가 적합한지 맞추는 과정에서 학습되는 모델\n",
    "#\"비행기를 타기 위해 000\"\n",
    "#\n",
    "#GPT1 : 1억1천7백만개의 가중치, GPT2 : 15억4천2백만 개의 가중치로 구성\n",
    "#문장 생성에 강점을 가진 모델\n",
    "#Transformer의 디코더만 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT(Bidirectional Encoder Representations from Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 2018년에 구글이 공개한 MASK 언어 모델\n",
    "#\n",
    "#MASK 언어 모델 : 문장 내 특정 위치에 빈칸을 만들고 해당 빈칸에 적합한 단어가 무엇일지 분류하는\n",
    "#과정에서 학습되는 모델\n",
    "#\n",
    "#\"비행기를 타기 위해 공항을 가는데 [MASK] 너무 막혀서 결국 비행기를 놓쳤음\"\n",
    "#\n",
    "#> 사람의 경우, 수많은 단어와 문장을 듣고 쓰고 말하며 언어능력을 학습했기 때문에,\n",
    "#확률적으로 '차가'라는 단어가 가장 적합하다고 판단할 수 있음\n",
    "#> 인공지능 : 매우 큰 모델에 많은 양의 데이터를 학습\n",
    "#\n",
    "#1억1천만 ~ 3억4천5백만 개의 가중치로 구성, 총 33억 단어를 학습시킨 모델\n",
    "#MASK 토큰 앞뒤 문맥을 모두 참고하는 양방향 언어 모델\n",
    "#문장의 의미를 추출하는데 강점을 가진 모델\n",
    "#Transformer의 인코더만 사용\n",
    "\n",
    "\n",
    "#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "\n",
    "#동작 과정(1) : text 입력 -> 토큰화 + Masking -> Token 임베딩 + Segment 임베딩 + Position 임베딩 -> 정수 인코딩 -> 임베딩 행렬 생성\n",
    "#\n",
    "#(예시) My dog is cute -> ['My', 'dog', 'is', 'cute'] -> [ [CLS], 'My', [MASK], 'is', 'cute', [SEP] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rollbackTarget",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
