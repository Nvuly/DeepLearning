{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentenceBERT + 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentenceBERT 모델을 이용하여 임베딩 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 필요한 라이브러리 설치 및 import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "import tensorflow as tf\n",
    "from transformers import pipeline, set_seed, BertTokenizer, TFBertForSequenceClassification\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 데이터 확인 : \n",
      "              id                                           document  label\n",
      "0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
      "1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
      "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
      "3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
      "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
      "...          ...                                                ...    ...\n",
      "149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n",
      "149996   8549745                                      평점이 너무 낮아서...      1\n",
      "149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
      "149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
      "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
      "\n",
      "[150000 rows x 3 columns]\n",
      "********************************************************************************\n",
      "평가용 데이터 확인 : \n",
      "            id                                           document  label\n",
      "0      6270596                                                굳 ㅋ      1\n",
      "1      9274899                               GDNTOPCLASSINTHECLUB      0\n",
      "2      8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
      "3      6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
      "4      6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0\n",
      "...        ...                                                ...    ...\n",
      "49995  4608761          오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 육쾌함      1\n",
      "49996  5308387       의지 박약들이나 하는거다 탈영은 일단 주인공 김대희 닮았고 이등병 찐따 OOOO      0\n",
      "49997  9072549                 그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다      0\n",
      "49998  5802125     절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네      0\n",
      "49999  6070594                                         마무리는 또 왜이래      0\n",
      "\n",
      "[50000 rows x 3 columns]\n",
      "********************************************************************************\n",
      "              id                                           document  label\n",
      "0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
      "1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
      "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
      "3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
      "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
      "...          ...                                                ...    ...\n",
      "149990   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n",
      "149991   8549745                                      평점이 너무 낮아서...      1\n",
      "149992   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
      "149993   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
      "149994   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
      "\n",
      "[149995 rows x 3 columns]\n",
      "********************************************************************************\n",
      "            id                                           document  label\n",
      "0      6270596                                                굳 ㅋ      1\n",
      "1      9274899                               GDNTOPCLASSINTHECLUB      0\n",
      "2      8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
      "3      6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
      "4      6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0\n",
      "...        ...                                                ...    ...\n",
      "49992  4608761          오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 육쾌함      1\n",
      "49993  5308387       의지 박약들이나 하는거다 탈영은 일단 주인공 김대희 닮았고 이등병 찐따 OOOO      0\n",
      "49994  9072549                 그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다      0\n",
      "49995  5802125     절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네      0\n",
      "49996  6070594                                         마무리는 또 왜이래      0\n",
      "\n",
      "[49997 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "### 학습용 데이터 --> DataFrame 생성, 누락 데이터 제거\n",
    "\n",
    "# Data Load\n",
    "\n",
    "file_path = ('D:\\Code\\DataSets/ratings_train.txt')\n",
    "df_train = pd.read_csv(file_path,sep='\\t')\n",
    "\n",
    "print(f'학습용 데이터 확인 : \\n{df_train}')\n",
    "\n",
    "print('*'*80)\n",
    "\n",
    "file_path = 'D:\\Code\\DataSets/ratings_test.txt' \n",
    "df_test = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "print(f'평가용 데이터 확인 : \\n{df_test}')\n",
    "\n",
    "# 각 컬럼별 누락 데이터의 수 확인\n",
    "num_nulls = df_train.isnull().sum()\n",
    "\n",
    "df_train.dropna(inplace=True)\n",
    "\n",
    "num_nulls2 = df_test.isnull().sum()\n",
    "\n",
    "df_test.dropna(inplace=True)\n",
    "\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('*'*80)\n",
    "\n",
    "\n",
    "print(df_train)\n",
    "\n",
    "print('*'*80)\n",
    "\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                                           document  label\n",
      "0        9976970                                아 더빙   진짜 짜증나네요 목소리      0\n",
      "1        3819312                  흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나      1\n",
      "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
      "3        9045019                      교도소 이야기구먼   솔직히 재미는 없다  평점 조정      0\n",
      "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
      "...          ...                                                ...    ...\n",
      "149990   6222902                                인간이 문제지   소는 뭔죄인가        0\n",
      "149991   8549745                                      평점이 너무 낮아서         1\n",
      "149992   9311800                    이게 뭐요  한국인은 거들먹거리고 필리핀 혼혈은 착하다       0\n",
      "149993   2376369                        청춘 영화의 최고봉 방황과 우울했던 날들의 자화상      1\n",
      "149994   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
      "\n",
      "[149995 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식 re를 쓰지않고 re.sub을 쓰지 않고 다르게 해보자!\n",
    "\n",
    "# pattern 생성 : 한글 글자. 자음, 모음, 공백을 제외한 나머지 제거\n",
    "\n",
    "import re\n",
    "pattern = '[^가-힣ㄱ-ㅎㅏ-ㅣ ]'\n",
    "\n",
    "# Series.str.replace(pattern, repl)\n",
    "#df_train.loc[:, 'document'] = df_train.loc[:,'document'].str.replace(pattern, ' ')\n",
    "df_train.loc[:, 'document'] = df_train.loc[:,'document'].apply(lambda x : re.sub(pattern, ' ', x))\n",
    "\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                           document  label\n",
      "0      6270596                                                굳 ㅋ      1\n",
      "1      9274899                               GDNTOPCLASSINTHECLUB      0\n",
      "2      8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
      "3      6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
      "4      6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0\n",
      "...        ...                                                ...    ...\n",
      "49992  4608761          오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 육쾌함      1\n",
      "49993  5308387       의지 박약들이나 하는거다 탈영은 일단 주인공 김대희 닮았고 이등병 찐따 OOOO      0\n",
      "49994  9072549                 그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다      0\n",
      "49995  5802125     절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네      0\n",
      "49996  6070594                                         마무리는 또 왜이래      0\n",
      "\n",
      "[49997 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pattern = '[^가-힣ㄱ-ㅎㅏ-ㅣ ]'\n",
    "\n",
    "#df_test.loc[:, 'document'] = df_train.loc[:,'document'].apply(lambda x : re.sub(pattern, ' ' , x))\n",
    "df_test.loc[:, 'document'] = df_test.loc[:,'document'].str.replace(pattern, ' ')\n",
    "\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 사전 학습된 한국어 SentenceBERT 모델 생성(다운로드)\n",
    "model_name='ddobokki/klue-roberta-base-nli-sts'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아 더빙   진짜 짜증나네요 목소리' '흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나'\n",
      " '너무재밓었다그래서보는것을추천한다' ... '이게 뭐요  한국인은 거들먹거리고 필리핀 혼혈은 착하다 '\n",
      " '청춘 영화의 최고봉 방황과 우울했던 날들의 자화상' '한국 영화 최초로 수간하는 내용이 담긴 영화']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentences)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# model.encode(sentences)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m train_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(sentences)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 결과 확인하기\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features)\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:844\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    835\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    837\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    838\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    839\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    842\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    843\u001b[0m )\n\u001b[1;32m--> 844\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    845\u001b[0m     embedding_output,\n\u001b[0;32m    846\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m    847\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    848\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    849\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m    850\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    851\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    852\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    853\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    854\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    855\u001b[0m )\n\u001b[0;32m    856\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    857\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:529\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    520\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    521\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    522\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    527\u001b[0m     )\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 529\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    530\u001b[0m         hidden_states,\n\u001b[0;32m    531\u001b[0m         attention_mask,\n\u001b[0;32m    532\u001b[0m         layer_head_mask,\n\u001b[0;32m    533\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    534\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    535\u001b[0m         past_key_value,\n\u001b[0;32m    536\u001b[0m         output_attentions,\n\u001b[0;32m    537\u001b[0m     )\n\u001b[0;32m    539\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:455\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    452\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    453\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 455\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[0;32m    457\u001b[0m )\n\u001b[0;32m    458\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:467\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 467\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m    468\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:365\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 365\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    366\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\human\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### 학습용 데이터(document) --> 문장 임베딩 행렬 생성 및 결과 저장하기\n",
    "\n",
    "#\n",
    "#def sentence_enc(sentence):\n",
    "#    return model.encode(sentence)\n",
    "\n",
    "# 너무 간단하니 lamba:x로 가능하지않을까? 사용자정의 함수를 굳이 하지 않는다?\n",
    "#\n",
    "#df_train.loc[:, 'document'].apply(lambda x:model.encode(x))\n",
    "\n",
    "# sentences 생성\n",
    "sentences = df_train.loc[:,'document'].values\n",
    "print(sentences)\n",
    "\n",
    "# model.encode(sentences)\n",
    "train_embeddings = model.encode(sentences)\n",
    "\n",
    "print('*'*80)\n",
    "\n",
    "# 결과 확인하기\n",
    "print(f'학습용 전체 리뷰에 대한 임베딩 행렬의 모양 : {train_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m train_embeddings_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDeep Learning/train_embeddings.npy \u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 결과 저장하기\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m np\u001b[38;5;241m.\u001b[39msave(train_embeddings_path, train_embeddings)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "### 학습용 데이터에 대한 임베딩 결과 저장하기\n",
    "\n",
    "train_embeddings_path = 'D:\\Code\\Deep Learning/train_embeddings.npy ' \n",
    "\n",
    "# 결과 저장하기\n",
    "np.save(train_embeddings_path, train_embeddings) #앞에는 경로 뒤에는 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 리뷰 전체에 대한 임베딩 행렬의 모양 : (149995, 768)\n",
      "********************************************************************************\n",
      "학습용 리뷰 전체에 대한 임베딩 행렬 확인 : \n",
      "[[ 0.22963014 -0.30546665 -0.0558625  ...  0.3554866   0.07065215\n",
      "   0.13408607]\n",
      " [ 0.10034887  0.51734966 -0.28517404 ...  0.05499696  0.08929399\n",
      "   0.04660222]\n",
      " [ 0.03501115 -0.59138674 -0.5399289  ...  0.07836973  0.6025016\n",
      "  -0.2706314 ]\n",
      " ...\n",
      " [ 0.16088536  0.3518497  -0.87538946 ...  0.75445354  0.28457245\n",
      "   0.36457962]\n",
      " [-0.27215552 -0.08595777  0.6485945  ... -0.12068029 -0.13909394\n",
      "  -0.07137752]\n",
      " [-0.33635998  0.4318327  -0.34545404 ...  0.10001148 -0.04830904\n",
      "  -0.37660566]]\n"
     ]
    }
   ],
   "source": [
    "### 학습용 데이터에 대한 임베딩 결과 불러오기\n",
    "\n",
    "# 파일 경로 설정 \n",
    "\n",
    "train_embeddings_path = 'D:\\Code\\Deep Learning/train_embeddings.npy ' \n",
    "\n",
    "# 저장된 학습용 임베딩 행렬 불러오기\n",
    "\n",
    "loaded_train_embeddings = np.load(train_embeddings_path)\n",
    "\n",
    "# 결과 확인하기\n",
    "print(f'학습용 리뷰 전체에 대한 임베딩 행렬의 모양 : {loaded_train_embeddings.shape}')\n",
    "print('*'*80)\n",
    "print(f'학습용 리뷰 전체에 대한 임베딩 행렬 확인 : \\n{loaded_train_embeddings}')\n",
    "\n",
    "##############mode.encode를 쓰면 다 배열로 바꿈. 2차원으로 바꿈."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가용 리뷰 전체에 대한 임베딩 행렬의 모양 : (49997, 768)\n",
      "********************************************************************************\n",
      "평가용 리뷰 전체에 대한 임베딩 행렬 확인 : \n",
      "[[-0.04891214 -0.3438344  -0.10130484 ... -0.14603318  0.22806278\n",
      "  -0.65479577]\n",
      " [ 0.2611376  -0.80607367 -0.6604893  ...  0.2817157   0.48219842\n",
      "  -0.31308427]\n",
      " [ 0.28456554  0.28490198  0.09374098 ... -0.15658678  0.53721684\n",
      "  -0.04801863]\n",
      " ...\n",
      " [-0.5774844   0.51906407  0.06169851 ...  0.01148296  0.5126719\n",
      "   0.99342686]\n",
      " [ 0.33191264 -0.07300376 -0.22728907 ...  0.2854041  -0.4874205\n",
      "  -0.03093443]\n",
      " [-0.09021662  0.4134429  -0.28334516 ...  0.13762715  0.32434598\n",
      "   1.0721738 ]]\n"
     ]
    }
   ],
   "source": [
    "### 평가용 데이터에 대한 임베딩 결과 불러오기\n",
    "\n",
    "# 파일 경로 설정 \n",
    "\n",
    "test_embeddings_path = 'D:\\Code\\Deep Learning/test_embeddings.npy ' \n",
    "\n",
    "# 저장된 학습용 임베딩 행렬 불러오기\n",
    "\n",
    "loaded_test_embeddings = np.load(test_embeddings_path, allow_pickle=True)\n",
    "\n",
    "# 결과 확인하기\n",
    "print(f'평가용 리뷰 전체에 대한 임베딩 행렬의 모양 : {loaded_test_embeddings.shape}')\n",
    "print('*'*80)\n",
    "print(f'평가용 리뷰 전체에 대한 임베딩 행렬 확인 : \\n{loaded_test_embeddings}')\n",
    "\n",
    "##############mode.encode를 쓰면 다 배열로 바꿈. 2차원으로 바꿈.\n",
    "\n",
    "################ 챗봇만들 때 얘가 필요함 ㅎㅎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4         5         6    \\\n",
      "0       0.229630 -0.305467 -0.055862 -0.810149  0.295727 -1.007614 -0.658901   \n",
      "1       0.100349  0.517350 -0.285174  0.700975 -0.242628 -0.459034 -0.250942   \n",
      "2       0.035011 -0.591387 -0.539929  0.306152 -0.052974 -0.767612 -0.155208   \n",
      "3       0.136597 -0.102633  0.427375  0.192248 -0.002059  0.413561  0.462351   \n",
      "4      -0.390766 -0.215280  0.269350  0.157344 -0.430786 -0.232554 -0.489062   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "149990 -0.371298 -0.146962  0.785210  0.437134 -0.018172  0.148512  0.532089   \n",
      "149991  0.222332  0.079697 -0.505798  0.067344 -0.052917 -0.058388 -0.149334   \n",
      "149992  0.160885  0.351850 -0.875389  0.616683  0.661793 -0.640041  0.424856   \n",
      "149993 -0.272156 -0.085958  0.648594  0.569168  0.259150  0.845531 -0.638321   \n",
      "149994 -0.336360  0.431833 -0.345454 -0.314272 -0.244533  0.343347 -0.345283   \n",
      "\n",
      "             7         8         9    ...       758       759       760  \\\n",
      "0       0.090830 -0.082614 -0.776525  ... -0.110274 -0.769546  0.016059   \n",
      "1       0.325110  0.067843 -0.130707  ...  0.137099 -0.753263 -0.112020   \n",
      "2      -0.219843  0.163960 -0.760903  ...  0.673305 -0.663921  0.421171   \n",
      "3       0.263610  0.602762  0.089575  ... -0.130281  0.154805 -0.293348   \n",
      "4       0.400542 -0.478659 -0.339678  ...  0.734956  0.143297 -0.209642   \n",
      "...          ...       ...       ...  ...       ...       ...       ...   \n",
      "149990 -0.250782  0.280532  0.070227  ... -0.648063 -0.417623 -0.234712   \n",
      "149991 -0.193747  0.179024 -0.080259  ... -0.282040  0.016285  0.164573   \n",
      "149992  0.175016  0.589952 -0.577956  ... -0.252386 -0.435127  0.431350   \n",
      "149993  0.957156  0.629194 -0.798687  ... -0.634258 -0.081602 -0.145247   \n",
      "149994 -0.648530  0.400296 -0.132604  ...  0.292068 -0.383131 -0.523391   \n",
      "\n",
      "             761       762       763       764       765       766       767  \n",
      "0      -0.154140  0.002403  0.278503  0.259754  0.355487  0.070652  0.134086  \n",
      "1       0.376907  0.523829  0.487437 -0.377845  0.054997  0.089294  0.046602  \n",
      "2      -0.389780 -0.672448  0.107734  0.080000  0.078370  0.602502 -0.270631  \n",
      "3      -0.000939  0.321190 -0.074750 -0.275169  0.196657 -0.374674  0.515594  \n",
      "4       0.337305  0.631904 -0.285955 -0.381186 -0.236698 -0.375010 -0.748669  \n",
      "...          ...       ...       ...       ...       ...       ...       ...  \n",
      "149990 -0.999511  0.046179  0.633019  0.286960  0.370683 -0.148808  0.410222  \n",
      "149991  0.357090 -0.386785 -0.615548 -0.030494 -0.059189  0.308430 -0.101827  \n",
      "149992 -0.306551  0.347571  0.057538  0.111611  0.754454  0.284572  0.364580  \n",
      "149993  0.136247 -0.290966  0.265197 -0.481685 -0.120680 -0.139094 -0.071378  \n",
      "149994 -0.159419  0.482015 -0.393438 -0.493861  0.100011 -0.048309 -0.376606  \n",
      "\n",
      "[149995 rows x 768 columns]\n"
     ]
    }
   ],
   "source": [
    "### X_train 생성\n",
    "\n",
    "X_train = pd.DataFrame(data=loaded_train_embeddings)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0\n",
      "1         1\n",
      "2         0\n",
      "3         0\n",
      "4         1\n",
      "         ..\n",
      "149990    0\n",
      "149991    1\n",
      "149992    0\n",
      "149993    1\n",
      "149994    0\n",
      "Name: label, Length: 149995, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### y_train 생성\n",
    "y_train = df_train.loc[:,'label']\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6    \\\n",
      "0     -0.048912 -0.343834 -0.101305 -0.078701  0.054339 -1.238898 -0.488888   \n",
      "1      0.261138 -0.806074 -0.660489  0.339362 -0.367431 -0.941752 -0.066228   \n",
      "2      0.284566  0.284902  0.093741 -0.017227  0.061789 -0.239323  0.268976   \n",
      "3     -0.016888 -0.335148 -0.017589  0.053198 -0.579792 -0.185808  0.243329   \n",
      "4      0.055227  0.631026  0.129269 -0.504613  0.160170  0.039439  0.226791   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "49992  0.562343  0.006787  0.393914 -0.142192 -0.193313  0.614337 -0.238424   \n",
      "49993 -0.413807  0.090225 -0.302836 -0.476721  0.023064  0.228728 -0.055091   \n",
      "49994 -0.577484  0.519064  0.061699  0.202573  0.003623  0.164711 -0.315304   \n",
      "49995  0.331913 -0.073004 -0.227289  0.235991 -0.669331  0.198543 -0.422011   \n",
      "49996 -0.090217  0.413443 -0.283345  0.265301  0.437998 -0.734049 -0.478744   \n",
      "\n",
      "            7         8         9    ...       758       759       760  \\\n",
      "0     -0.324756  0.150131  0.127012  ...  0.712760 -0.358766  0.835138   \n",
      "1     -0.259148 -0.154754 -0.700186  ...  0.856341 -0.450144  0.309163   \n",
      "2     -0.149219 -0.162338 -0.542940  ...  0.210067  0.588997 -0.156488   \n",
      "3     -0.032935 -0.128916 -0.340554  ...  0.289231  0.168294 -0.693247   \n",
      "4     -0.359864 -0.112177 -0.290653  ... -1.264610  0.008181  0.214807   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "49992  0.464128 -0.040681 -0.079709  ...  0.188862  0.159461  0.043849   \n",
      "49993 -0.235303 -0.692514 -0.200041  ... -0.319378 -0.292925  0.144229   \n",
      "49994 -0.613331  0.405977 -0.628459  ... -0.653857 -0.144688  0.148023   \n",
      "49995 -0.037091  0.426708 -0.308556  ... -0.007615 -0.053263 -0.422674   \n",
      "49996 -0.172166 -0.361438 -0.408368  ... -0.328850 -0.505885  0.112613   \n",
      "\n",
      "            761       762       763       764       765       766       767  \n",
      "0      0.100108 -0.981759  0.488757  0.131579 -0.146033  0.228063 -0.654796  \n",
      "1     -0.488123 -0.525554 -0.081450  0.204163  0.281716  0.482198 -0.313084  \n",
      "2     -0.435905  0.568382 -0.155172 -0.382814 -0.156587  0.537217 -0.048019  \n",
      "3      0.452223  0.555937  0.294284 -0.441232 -0.380247 -0.578392  0.526958  \n",
      "4     -0.255596 -0.224104 -0.311117  0.100338 -0.183139  0.002153 -0.331638  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "49992  0.132810  0.211333 -0.071861 -0.427835 -0.106293  0.185837 -0.428412  \n",
      "49993 -0.229304 -0.403254  0.821349 -0.759487  0.455623 -0.513002 -0.306485  \n",
      "49994 -0.121221  0.302777  0.056876  0.156198  0.011483  0.512672  0.993427  \n",
      "49995  0.184811  1.062947  0.312096  0.235775  0.285404 -0.487420 -0.030934  \n",
      "49996 -0.272856 -0.382081  0.332986 -0.423952  0.137627  0.324346  1.072174  \n",
      "\n",
      "[49997 rows x 768 columns]\n"
     ]
    }
   ],
   "source": [
    "### X_test 생성\n",
    "X_test = pd.DataFrame(data=loaded_test_embeddings)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "49992    1\n",
      "49993    0\n",
      "49994    0\n",
      "49995    0\n",
      "49996    0\n",
      "Name: label, Length: 49997, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_test = df_test.loc[:,'label']\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM 모델을 이용한 텍스트 감성 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 74825, number of negative: 75170\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.256996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 149995, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498850 -> initscore=-0.004600\n",
      "[LightGBM] [Info] Start training from score -0.004600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(n_estimators=300, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(n_estimators=300, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(n_estimators=300, random_state=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = LGBMClassifier(learning_rate=0.1,\n",
    "                           n_estimators=300,\n",
    "                           random_state=0\n",
    "                           )\n",
    "\n",
    "lgbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가용 데이터를 이용한 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = lgbm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가용 데이터에 대한 성능 평가 결과 : \n",
      "   accuracy  f1_score  precision    recall\n",
      "0   0.83905  0.839449   0.843166  0.835763\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def eval_model(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    data = [[accuracy, f1, precision, recall]]\n",
    "    columns = ['accuracy', 'f1_score', 'precision', 'recall']\n",
    "    eval_df = pd.DataFrame(data=data, columns=columns)\n",
    "    return eval_df\n",
    "\n",
    "result = eval_model(y_true=y_test, y_pred=pred_test)\n",
    "\n",
    "print(f'평가용 데이터에 대한 성능 평가 결과 : \\n{result}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NvulyTarget",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
